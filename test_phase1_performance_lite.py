#!/usr/bin/env python3
"""
Phase 1 Performance Assessment Script - Lite Version

Performance testing for Days 1-3 implementation without FUSE dependencies:
- Speed layer baseline measurement  
- Component initialization timing
- Memory usage analysis
- Concurrent validation testing

Tests performance readiness for 99% <100ms SLA validation requirement.
"""

import asyncio
import json
import logging
import psutil
import statistics
import time
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Optional, Any
import unittest.mock

# Configure logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# Measurements storage
measurements = []

class PerformanceTimer:
    """Context manager for performance timing"""
    
    def __init__(self, operation_name: str, metadata: Optional[Dict[str, Any]] = None):
        self.operation_name = operation_name
        self.metadata = metadata or {}
        self.start_time = None
        self.end_time = None
        self.process = psutil.Process()
        self.start_memory = None
        self.start_cpu = None
    
    def __enter__(self):
        self.start_memory = self.process.memory_info().rss / 1024 / 1024  # MB
        self.start_cpu = self.process.cpu_percent()
        self.start_time = time.time()
        return self
    
    def __exit__(self, exc_type, exc_val, exc_tb):
        self.end_time = time.time()
        duration_ms = (self.end_time - self.start_time) * 1000
        
        end_memory = self.process.memory_info().rss / 1024 / 1024  # MB
        memory_delta = end_memory - self.start_memory
        
        measurement = {
            'timestamp': datetime.now().isoformat(),
            'operation': self.operation_name,
            'duration_ms': duration_ms,
            'memory_start_mb': self.start_memory,
            'memory_end_mb': end_memory,
            'memory_delta_mb': memory_delta,
            'cpu_percent': self.process.cpu_percent(),
            'metadata': self.metadata
        }
        
        measurements.append(measurement)
        logger.info(f"{self.operation_name}: {duration_ms:.2f}ms, Memory: {memory_delta:+.1f}MB")


async def test_speed_layer_performance():
    """Test speed layer component performance"""
    logger.info("=== Testing Speed Layer Performance ===")
    
    try:
        from lighthouse.bridge.speed_layer import OptimizedSpeedLayerDispatcher
        from lighthouse.bridge.speed_layer.models import ValidationRequest
    except ImportError as e:
        logger.error(f"Failed to import speed layer components: {e}")
        return False
    
    # Initialize dispatcher
    with PerformanceTimer("speed_layer_init", {"component": "dispatcher"}):
        dispatcher = OptimizedSpeedLayerDispatcher(
            max_memory_cache_size=1000,
            expert_timeout=10.0
        )
    
    # Start dispatcher
    with PerformanceTimer("speed_layer_start", {"component": "dispatcher"}):
        await dispatcher.start()
    
    try:
        # Test safe command validation
        safe_request = ValidationRequest(
            tool_name="Read",
            tool_input={"file_path": "/tmp/test.txt"},
            agent_id="perf_test_agent"
        )
        
        with PerformanceTimer("safe_command_validation", {"component": "speed_layer", "command_type": "safe"}):
            result = await dispatcher.validate_request(safe_request)
        
        logger.info(f"Safe command result: {result.decision}, Cache: {result.cache_hit}")
        
        # Test risky command validation
        risky_request = ValidationRequest(
            tool_name="Bash",
            tool_input={"command": "rm /tmp/test_file"},
            agent_id="perf_test_agent"
        )
        
        with PerformanceTimer("risky_command_validation", {"component": "speed_layer", "command_type": "risky"}):
            result = await dispatcher.validate_request(risky_request)
        
        logger.info(f"Risky command result: {result.decision}, Cache: {result.cache_hit}")
        
        # Test dangerous command validation
        dangerous_request = ValidationRequest(
            tool_name="Bash", 
            tool_input={"command": "rm -rf /"},
            agent_id="perf_test_agent"
        )
        
        with PerformanceTimer("dangerous_command_validation", {"component": "speed_layer", "command_type": "dangerous"}):
            result = await dispatcher.validate_request(dangerous_request)
        
        logger.info(f"Dangerous command result: {result.decision}, Cache: {result.cache_hit}")
        
        # Test cache hit performance (repeat safe command)
        with PerformanceTimer("cache_hit_validation", {"component": "speed_layer", "command_type": "cached"}):
            result = await dispatcher.validate_request(safe_request)
        
        logger.info(f"Cached command result: {result.decision}, Cache: {result.cache_hit}")
        
    finally:
        # Stop dispatcher
        with PerformanceTimer("speed_layer_stop", {"component": "dispatcher"}):
            await dispatcher.stop()
    
    return True


async def test_concurrent_validation():
    """Test concurrent validation performance"""
    logger.info("=== Testing Concurrent Validation Performance ===")
    
    from lighthouse.bridge.speed_layer import OptimizedSpeedLayerDispatcher
    from lighthouse.bridge.speed_layer.models import ValidationRequest
    
    dispatcher = OptimizedSpeedLayerDispatcher(max_memory_cache_size=1000)
    await dispatcher.start()
    
    try:
        # Create test requests (realistic workload: 70% safe, 20% risky, 10% dangerous)
        test_requests = []
        for i in range(100):  # 100 concurrent requests for baseline
            if i < 70:  # 70% safe commands
                request = ValidationRequest(
                    tool_name="Read",
                    tool_input={"file_path": f"/tmp/test_{i}.txt"},
                    agent_id=f"agent_{i}"
                )
            elif i < 90:  # 20% risky commands (file operations)
                request = ValidationRequest(
                    tool_name="Write",
                    tool_input={"file_path": f"/tmp/output_{i}.txt", "content": "test"},
                    agent_id=f"agent_{i}"
                )
            else:  # 10% dangerous commands
                commands = ["rm -f /tmp/test", "sudo ls", "chmod 777 /tmp", "cat /etc/passwd"]
                request = ValidationRequest(
                    tool_name="Bash",
                    tool_input={"command": commands[i % len(commands)]},
                    agent_id=f"agent_{i}"
                )
            test_requests.append(request)
        
        # Concurrent validation test
        with PerformanceTimer("concurrent_validation", {"component": "speed_layer", "concurrent_requests": len(test_requests)}):
            # Execute all requests concurrently
            results = await asyncio.gather(*[
                dispatcher.validate_request(req) for req in test_requests
            ])
        
        # Analyze results
        decisions = [r.decision.value for r in results]
        response_times = [r.processing_time_ms for r in results]
        cache_hits = sum(1 for r in results if r.cache_hit)
        
        # Check SLA compliance
        slow_requests = [t for t in response_times if t > 100]
        sla_compliance = (len(response_times) - len(slow_requests)) / len(response_times)
        
        logger.info(f"Concurrent validation results:")
        logger.info(f"  - Total requests: {len(results)}")
        logger.info(f"  - Cache hits: {cache_hits} ({cache_hits/len(results)*100:.1f}%)")
        logger.info(f"  - Response times - Mean: {statistics.mean(response_times):.2f}ms, P95: {statistics.quantiles(response_times, n=20)[18]:.2f}ms")
        logger.info(f"  - SLA compliance: {sla_compliance*100:.1f}% (<100ms)")
        logger.info(f"  - Decisions: {dict(zip(*zip(*[[d, decisions.count(d)] for d in set(decisions)])))}") 
        
        # Store SLA compliance in metadata
        for measurement in measurements:
            if measurement['operation'] == 'concurrent_validation':
                measurement['metadata']['sla_compliance'] = sla_compliance
                measurement['metadata']['mean_response_time'] = statistics.mean(response_times)
                measurement['metadata']['p95_response_time'] = statistics.quantiles(response_times, n=20)[18]
                break
        
    finally:
        await dispatcher.stop()
    
    return True


async def test_memory_cache_performance():
    """Test optimized memory cache performance"""
    logger.info("=== Testing Memory Cache Performance ===")
    
    from lighthouse.bridge.speed_layer.optimized_memory_cache import OptimizedMemoryCache
    from lighthouse.bridge.speed_layer.models import ValidationResult, ValidationDecision, ValidationConfidence
    
    # Initialize cache
    with PerformanceTimer("memory_cache_init", {"component": "memory_cache"}):
        cache = OptimizedMemoryCache(max_size=1000)
    
    # Create test validation result
    test_result = ValidationResult(
        decision=ValidationDecision.APPROVED,
        confidence=ValidationConfidence.HIGH,
        reason="Test validation result",
        request_id="test_123",
        processing_time_ms=1.0
    )
    
    # Test cache set operations
    set_times = []
    for i in range(100):
        start = time.time()
        await cache.set(f"test_key_{i}", test_result, ttl_seconds=300)
        duration = (time.time() - start) * 1000
        set_times.append(duration)
    
    with PerformanceTimer("memory_cache_set", {"component": "memory_cache", "operation_count": 100}):
        pass  # Just for timing the overall operation
    
    # Test cache get operations
    get_times = []
    for i in range(100):
        start = time.time()
        result = await cache.get(f"test_key_{i}")
        duration = (time.time() - start) * 1000
        get_times.append(duration)
        if result is None:
            logger.warning(f"Cache miss for test_key_{i}")
    
    with PerformanceTimer("memory_cache_get", {"component": "memory_cache", "operation_count": 100}):
        pass
    
    # Test cache miss performance
    miss_times = []
    for i in range(10):
        start = time.time()
        result = await cache.get(f"nonexistent_key_{i}")
        duration = (time.time() - start) * 1000
        miss_times.append(duration)
    
    with PerformanceTimer("memory_cache_miss", {"component": "memory_cache", "operation_count": 10}):
        pass
    
    logger.info(f"Cache performance:")
    logger.info(f"  - Set operations: {statistics.mean(set_times):.3f}ms avg, {max(set_times):.3f}ms max")
    logger.info(f"  - Get operations: {statistics.mean(get_times):.3f}ms avg, {max(get_times):.3f}ms max")
    logger.info(f"  - Miss operations: {statistics.mean(miss_times):.3f}ms avg, {max(miss_times):.3f}ms max")
    
    # Get cache statistics
    stats = cache.get_stats()
    logger.info(f"  - Cache stats: {stats}")
    
    return True


async def test_event_store_performance():
    """Test event store performance"""
    logger.info("=== Testing Event Store Performance ===")
    
    from lighthouse.event_store import EventStore
    
    # Initialize event store
    with PerformanceTimer("event_store_init", {"component": "event_store"}):
        event_store = EventStore()
    
    # Test single event append performance
    with PerformanceTimer("event_store_single_append", {"component": "event_store"}):
        await event_store.append_event(
            aggregate_id="perf_test",
            event_type="test_event",
            event_data={"test": "data", "timestamp": time.time()}
        )
    
    # Test batch event append performance
    events = []
    for i in range(100):
        events.append({
            "aggregate_id": f"perf_test_{i}",
            "event_type": "batch_test_event",
            "event_data": {"index": i, "data": f"test_data_{i}", "timestamp": time.time()}
        })
    
    with PerformanceTimer("event_store_batch_append", {"component": "event_store", "event_count": 100}):
        for event in events:
            await event_store.append_event(**event)
    
    # Test event retrieval performance
    with PerformanceTimer("event_store_retrieve", {"component": "event_store"}):
        events = await event_store.get_events("perf_test")
    
    logger.info(f"Retrieved {len(events)} events from event store")
    
    return True


async def test_component_startup_performance():
    """Test individual component startup performance"""
    logger.info("=== Testing Component Startup Performance ===")
    
    # Test speed layer components
    with PerformanceTimer("optimized_memory_cache_startup", {"component": "optimized_memory_cache"}):
        from lighthouse.bridge.speed_layer.optimized_memory_cache import OptimizedMemoryCache
        cache = OptimizedMemoryCache(max_size=10000)
    
    with PerformanceTimer("optimized_policy_cache_startup", {"component": "optimized_policy_cache"}):
        from lighthouse.bridge.speed_layer.optimized_policy_cache import OptimizedPolicyCache
        policy_cache = OptimizedPolicyCache()
    
    with PerformanceTimer("optimized_pattern_cache_startup", {"component": "optimized_pattern_cache"}):
        from lighthouse.bridge.speed_layer.optimized_pattern_cache import OptimizedPatternCache
        pattern_cache = OptimizedPatternCache()
    
    # Test event store components
    with PerformanceTimer("project_aggregate_startup", {"component": "project_aggregate"}):
        from lighthouse.bridge.event_store.project_aggregate import ProjectAggregate
        project_aggregate = ProjectAggregate("startup_test")
    
    # Test expert coordination components (without FUSE)
    with PerformanceTimer("expert_coordinator_startup", {"component": "expert_coordinator"}):
        from lighthouse.bridge.expert_coordination.coordinator import ExpertCoordinator
        from lighthouse.event_store import EventStore
        event_store = EventStore()
        coordinator = ExpertCoordinator(event_store)
    
    return True


async def analyze_performance_results():
    """Analyze collected performance measurements"""
    logger.info("=== Performance Analysis ===")
    
    if not measurements:
        logger.error("No performance measurements collected!")
        return False
    
    # Group measurements by operation
    operations = {}
    for m in measurements:
        op = m['operation']
        if op not in operations:
            operations[op] = []
        operations[op].append(m)
    
    results = {}
    
    for op_name, op_measurements in operations.items():
        durations = [m['duration_ms'] for m in op_measurements]
        memory_deltas = [m['memory_delta_mb'] for m in op_measurements]
        
        if durations:
            results[op_name] = {
                'count': len(durations),
                'duration_mean': statistics.mean(durations),
                'duration_min': min(durations),
                'duration_max': max(durations),
                'memory_delta_mean': statistics.mean(memory_deltas) if memory_deltas else 0,
                'sample_metadata': op_measurements[0].get('metadata', {})
            }
            
            if len(durations) > 1:
                results[op_name]['duration_stdev'] = statistics.stdev(durations)
            
            if len(durations) >= 4:
                quantiles = statistics.quantiles(durations, n=4)
                results[op_name]['duration_p75'] = quantiles[2]
                if len(durations) >= 20:
                    results[op_name]['duration_p95'] = durations[int(len(durations) * 0.95)]
    
    # Print results
    logger.info("\n=== PHASE 1 PERFORMANCE RESULTS ===")
    
    critical_operations = [
        'speed_layer_init', 'speed_layer_start',
        'safe_command_validation', 'risky_command_validation', 'dangerous_command_validation', 'cache_hit_validation',
        'concurrent_validation',
        'memory_cache_init', 'memory_cache_set', 'memory_cache_get', 'memory_cache_miss',
        'event_store_init', 'event_store_single_append', 'event_store_batch_append'
    ]
    
    sla_violations = []
    performance_issues = []
    
    for op_name in critical_operations:
        if op_name in results:
            r = results[op_name]
            duration = r['duration_mean']
            
            # Check SLA compliance (different thresholds for different operations)
            if op_name in ['safe_command_validation', 'risky_command_validation', 'dangerous_command_validation', 'cache_hit_validation']:
                sla_threshold = 100.0  # 100ms for validation operations
                sla_compliant = duration < sla_threshold
                if not sla_compliant:
                    sla_violations.append(f"{op_name}: {duration:.2f}ms (>{sla_threshold}ms)")
            elif op_name in ['memory_cache_get', 'memory_cache_set', 'memory_cache_miss']:
                sla_threshold = 1.0  # 1ms for cache operations
                sla_compliant = duration < sla_threshold
                if not sla_compliant:
                    performance_issues.append(f"{op_name}: {duration:.2f}ms (>{sla_threshold}ms)")
            elif op_name == 'concurrent_validation':
                # Check SLA compliance metadata
                metadata = r['sample_metadata']
                sla_compliance = metadata.get('sla_compliance', 0.0)
                sla_compliant = sla_compliance >= 0.99  # 99% SLA compliance
                if not sla_compliant:
                    sla_violations.append(f"{op_name}: {sla_compliance*100:.1f}% SLA compliance (<99%)")
            else:
                sla_compliant = True  # No strict SLA for initialization operations
            
            status = "‚úÖ" if sla_compliant else "‚ùå"
            
            logger.info(f"{status} {op_name:30s}: {duration:8.2f}ms (¬±{r.get('duration_stdev', 0):5.2f}ms) Memory: {r['memory_delta_mean']:+6.1f}MB")
    
    # Overall assessment
    logger.info(f"\n=== PHASE 1 PERFORMANCE ASSESSMENT ===")
    
    performance_score = 10.0
    
    if sla_violations:
        logger.error("‚ùå CRITICAL SLA VIOLATIONS DETECTED:")
        for violation in sla_violations:
            logger.error(f"   {violation}")
        performance_score -= 2.0 * len(sla_violations)
    
    if performance_issues:
        logger.warning("‚ö†Ô∏è  PERFORMANCE ISSUES DETECTED:")
        for issue in performance_issues:
            logger.warning(f"   {issue}")
        performance_score -= 0.5 * len(performance_issues)
    
    if not sla_violations and not performance_issues:
        logger.info("‚úÖ All critical operations meet performance targets")
    
    # Additional analysis
    if 'concurrent_validation' in results:
        concurrent_result = results['concurrent_validation']
        concurrent_duration = concurrent_result['duration_mean']
        metadata = concurrent_result['sample_metadata']
        request_count = metadata.get('concurrent_requests', 'unknown')
        sla_compliance = metadata.get('sla_compliance', 0.0)
        mean_response = metadata.get('mean_response_time', 0.0)
        p95_response = metadata.get('p95_response_time', 0.0)
        
        logger.info(f"üöÄ Concurrent validation summary:")
        logger.info(f"   - Total requests: {request_count}")
        logger.info(f"   - Total duration: {concurrent_duration:.2f}ms")
        logger.info(f"   - Mean response time: {mean_response:.2f}ms")
        logger.info(f"   - P95 response time: {p95_response:.2f}ms")
        logger.info(f"   - SLA compliance: {sla_compliance*100:.1f}%")
        
        if p95_response > 50:
            logger.warning("‚ö†Ô∏è  P95 response time higher than optimal")
            performance_score -= 0.25
    
    # Check memory efficiency
    total_memory_delta = sum(r['memory_delta_mean'] for r in results.values())
    if total_memory_delta > 200:  # More than 200MB total memory increase
        logger.warning(f"‚ö†Ô∏è  High memory usage: +{total_memory_delta:.1f}MB total")
        performance_score -= 0.5
    
    # Cap the score
    performance_score = max(0.0, min(10.0, performance_score))
    
    # Determine readiness
    if performance_score >= 8.0:
        readiness_status = "READY"
        readiness_emoji = "üéâ"
    elif performance_score >= 7.0:
        readiness_status = "MOSTLY READY"
        readiness_emoji = "‚ö†Ô∏è"
    else:
        readiness_status = "NOT READY"
        readiness_emoji = "‚ùå"
    
    logger.info(f"\n{readiness_emoji} PHASE 1 PERFORMANCE SCORE: {performance_score:.1f}/10 - {readiness_status}")
    
    # Save detailed results
    results_file = Path(__file__).parent / "phase1_performance_results.json"
    with open(results_file, 'w') as f:
        json.dump({
            'timestamp': datetime.now().isoformat(),
            'performance_score': performance_score,
            'readiness_status': readiness_status,
            'sla_violations': sla_violations,
            'performance_issues': performance_issues,
            'operation_results': results,
            'raw_measurements': measurements
        }, f, indent=2)
    
    logger.info(f"üìä Detailed results saved to: {results_file}")
    
    return performance_score >= 7.0


async def main():
    """Run complete Phase 1 performance assessment"""
    logger.info("üöÄ Starting Phase 1 Performance Assessment (Lite Version)...")
    
    start_time = time.time()
    
    try:
        # Run all performance tests
        tests_passed = 0
        total_tests = 7
        
        if await test_speed_layer_performance():
            tests_passed += 1
        
        if await test_concurrent_validation():
            tests_passed += 1
        
        if await test_memory_cache_performance():
            tests_passed += 1
        
        if await test_event_store_performance():
            tests_passed += 1
        
        if await test_component_startup_performance():
            tests_passed += 1
        
        if await analyze_performance_results():
            tests_passed += 1
        
        # Final assessment
        total_duration = time.time() - start_time
        logger.info(f"\n‚è±Ô∏è  Total assessment time: {total_duration:.2f}s")
        logger.info(f"‚úÖ Tests passed: {tests_passed}/{total_tests}")
        
        if tests_passed >= 6:  # Allow one test failure
            logger.info("üéâ Phase 1 performance assessment PASSED")
            return True
        else:
            logger.error("‚ùå Phase 1 performance assessment FAILED")
            return False
    
    except Exception as e:
        logger.error(f"üí• Performance assessment failed with error: {e}")
        import traceback
        traceback.print_exc()
        return False


if __name__ == "__main__":
    import sys
    success = asyncio.run(main())
    sys.exit(0 if success else 1)